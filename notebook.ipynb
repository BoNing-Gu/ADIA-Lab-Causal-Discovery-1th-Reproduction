{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import typing\n",
    "import pickle\n",
    "import functools\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocess import preprocess_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23500 23500 23500\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_pickle('data/X_train.pickle')\n",
    "y_train = pd.read_pickle('data/y_train.pickle')\n",
    "\n",
    "file_path = 'data/X_train_processed.npz'\n",
    "if os.path.exists(file_path):\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    X_train_processed = {key: data[key] for key in data}\n",
    "else:\n",
    "    X_train_processed = preprocess_dict(X_train, n_workers=32)\n",
    "    np.savez_compressed(file_path, **X_train_processed)\n",
    "\n",
    "print(len(X_train), len(X_train_processed), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 3, 1000)\n",
      "torch.Size([90, 3, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_processed['00000'].shape)\n",
    "edge_features = torch.tensor(X_train_processed['00000'], dtype=torch.float32)\n",
    "print(edge_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0  Y  2  3  4  5  6  7  8  X\n",
      "parent                              \n",
      "0       0  0  0  0  0  0  0  0  0  0\n",
      "Y       1  0  0  0  0  0  0  0  0  0\n",
      "2       0  0  0  0  0  1  0  0  0  0\n",
      "3       0  1  0  0  0  1  0  0  0  0\n",
      "4       0  1  0  0  0  1  0  0  0  0\n",
      "5       1  1  0  0  0  0  0  0  0  0\n",
      "6       0  1  0  0  0  1  0  0  0  0\n",
      "7       0  1  0  0  0  1  0  0  0  0\n",
      "8       0  0  1  1  0  0  0  1  0  1\n",
      "X       0  1  0  0  0  1  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "X_sample = pd.read_csv('data/A.csv', index_col=0)\n",
    "y_sample = pd.read_csv('data/B.csv', index_col=0)\n",
    "print(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for DAG\n",
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        X_dict=typing.Dict[str, pd.DataFrame], \n",
    "        X_processed_dict=typing.Dict[str, pd.DataFrame], # Preprocessed data\n",
    "        y_dict=typing.Dict[str, pd.DataFrame], \n",
    "        x_var='X', \n",
    "        y_var='Y'\n",
    "    ):\n",
    "        self.X_dict = X_dict\n",
    "        self.X_processed_dict = X_processed_dict\n",
    "        self.y_dict = y_dict\n",
    "        self.ids = list(X_dict.keys())\n",
    "        self.x_var = x_var\n",
    "        self.y_var = y_var\n",
    "        self.adjacency_graph, self.adjacency_label = create_graph_label()\n",
    "        self.node_labels = [\n",
    "            'Confounder', 'Collider', 'Mediator', 'Independent',\n",
    "            'Cause of X', 'Consequence of X', 'Cause of Y', 'Consequence of Y',\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.ids[idx]\n",
    "        X_sample = self.X_dict[sample_id]                      # DataFrame (data: 1000 * num_nodes)\n",
    "        X_processed_sample = self.X_processed_dict[sample_id]  # numpy array (data: num_edges * 3 * 1000)\n",
    "        y_sample = self.y_dict[sample_id]                      # DataFrame (adjacency matrix: num_nodes * num_nodes)\n",
    "\n",
    "        variables = X_sample.columns.tolist()\n",
    "        edge_features, edge_types = self._process_edges(X_processed_sample, variables)\n",
    "        node_labels = self._process_node_labels(y_sample, variables)\n",
    "        edge_labels = self._process_edge_labels(y_sample, variables)\n",
    "\n",
    "        return {\n",
    "            'edge_features': edge_features,  # (num_edges, 3, 1000)\n",
    "            'edge_types': edge_types,        # (num_edges,)\n",
    "            'node_labels': node_labels,      # (num_nodes - 2, 8)    # For Node CLF\n",
    "            'edge_labels': edge_labels,      # (num_edges, 2)        # For Edge CLF\n",
    "            'variables': variables           # List[str]\n",
    "        }\n",
    "    \n",
    "    def _process_edges(self, X_processed_sample, variables):\n",
    "        edge_features = []\n",
    "        edge_types = []\n",
    "        for u in variables:\n",
    "            for v in variables:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                edge_types.append(self._get_edge_type(u, v))\n",
    "\n",
    "        edge_features = torch.tensor(X_processed_sample, dtype=torch.float32)\n",
    "        edge_types = torch.tensor(edge_types, dtype=torch.long)\n",
    "        return edge_features, edge_types\n",
    "\n",
    "    def _get_edge_type(self, u, v):\n",
    "        x, y = self.x_var, self.y_var\n",
    "        if u == x and v != y:      # u is X but v is not Y\n",
    "            return 0\n",
    "        elif u == y and v != x:    # u is Y but v is not X\n",
    "            return 1\n",
    "        elif u != y and v == x:    # u is not Y but v is X \n",
    "            return 2\n",
    "        elif u != x and v == y:    # u is not X but v is Y \n",
    "            return 3\n",
    "        elif u == x and v == y:    # u is X and v is Y\n",
    "            return 4\n",
    "        elif u == y and v == x:    # u is Y and v is X\n",
    "            return 5\n",
    "        else:                      # none of the above\n",
    "            return 6\n",
    "        \n",
    "    def _process_node_labels(self, y_sample, variables):\n",
    "        node_label_dict = get_labels(y_sample, self.adjacency_label)\n",
    "        \n",
    "        # 剔除 x_var 和 y_var，得到 nodes\n",
    "        nodes = [var for var in variables if var not in {self.x_var, self.y_var}]\n",
    "\n",
    "        # 初始化 0/1 矩阵，大小为 len(nodes) * len(self.node_labels)\n",
    "        node_label_matrix = np.zeros((len(nodes), len(self.node_labels)), dtype=int)\n",
    "\n",
    "        # 遍历 nodes，并根据 node_label_dict 填充矩阵\n",
    "        for i, node in enumerate(nodes):\n",
    "            if node in node_label_dict:\n",
    "                label = node_label_dict[node]\n",
    "                if label in self.node_labels:\n",
    "                    j = self.node_labels.index(label)  # 获取标签对应的索引\n",
    "                    node_label_matrix[i, j] = 1    # 设置为 1\n",
    "        return torch.tensor(node_label_matrix, dtype=torch.long)\n",
    "    \n",
    "    def _process_edge_labels(self, y_sample, variables):\n",
    "        edge_label_matrix = []\n",
    "        for u in variables:\n",
    "            for v in variables:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                label_vector = np.zeros(2, dtype=int)\n",
    "                if y_sample.loc[u, v] == 1:\n",
    "                    label_vector[1] = 1\n",
    "                else:\n",
    "                    label_vector[0] = 1\n",
    "                edge_label_matrix.append(label_vector)\n",
    "        edge_label_matrix = np.array(edge_label_matrix)\n",
    "        return torch.tensor(edge_label_matrix, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            channels, channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(8, channels)  # 8 groups for 64 channels\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + identity\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=64, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # add sequence dimension\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        return self.norm(x.squeeze(1) + attn_output)\n",
    "    \n",
    "class MergeBlock(nn.Module):\n",
    "    def __init__(self, input_dim=256, output_dim=64):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        concatenated = torch.cat(embeddings, dim=-1)\n",
    "        return self.activation(self.norm(self.linear(concatenated)))\n",
    "\n",
    "class CausalModel(nn.Module):\n",
    "    def __init__(self, x_var='X', y_var='Y', hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.x_var = x_var\n",
    "        self.y_var = y_var\n",
    "        \n",
    "        # Stem layer\n",
    "        self.stem = nn.Conv1d(3, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        self.conv_blocks = nn.Sequential(*[\n",
    "            ConvBlock(hidden_dim) for _ in range(5)\n",
    "        ])\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Edge type embedding\n",
    "        self.edge_type_embed = nn.Embedding(7, hidden_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attns = nn.Sequential(*[\n",
    "            SelfAttentionBlock(hidden_dim) for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Classification heads\n",
    "        self.edge_cls = nn.Linear(hidden_dim, 2)\n",
    "        self.node_merge = MergeBlock(4*hidden_dim)\n",
    "        self.node_cls = nn.Linear(hidden_dim, 8)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Unpack batch\n",
    "        edge_features = batch['edge_features'].squeeze(0)  # [E, 3, 1000]\n",
    "        edge_types = batch['edge_types'].squeeze(0)        # [E]\n",
    "        variables = batch['variables']                     # list of var names\n",
    "        variables = [item for sublist in variables for item in sublist]  # Flatten list\n",
    "        # print(variables)\n",
    "        # print(self.x_var, self.y_var)\n",
    "        # print(variables.index(self.x_var), variables.index(self.y_var))\n",
    "        \n",
    "        # Feature extraction\n",
    "        x = self.stem(edge_features)            # [E, 64, 1000]\n",
    "        # print('After stem:', x.shape)\n",
    "        x = self.conv_blocks(x)                 # [E, 64, 1000]\n",
    "        # print('After conv blocks: ', x.shape)\n",
    "        x = self.pool(x).squeeze(-1)            # [E, 64]\n",
    "        # print('After pool: ', x.shape)\n",
    "        \n",
    "        # Add edge type embeddings\n",
    "        x = x + self.edge_type_embed(edge_types)\n",
    "        # print('After edge type embed: ', x.shape)\n",
    "        \n",
    "        # Self-attention\n",
    "        x = self.self_attns(x)                  # [E, 64]\n",
    "        # print('After self attns: ', x.shape)\n",
    "        \n",
    "        # Edge classification\n",
    "        edge_logits = self.edge_cls(x)          # [E, 2]\n",
    "        # print('Edge logits: ', edge_logits)\n",
    "        \n",
    "        # Node classification\n",
    "        p = len(variables)\n",
    "        edges = [(u, v) for u in range(p) for v in range(p) if u != v]\n",
    "        edge_indices = {(u, v): idx for idx, (u, v) in enumerate(edges)}\n",
    "        node_embs = []\n",
    "        try:\n",
    "            x_idx = variables.index(self.x_var)\n",
    "            y_idx = variables.index(self.y_var)\n",
    "        except ValueError:\n",
    "            return edge_logits, torch.tensor([])\n",
    "        \n",
    "        for u_idx, u in enumerate(variables):\n",
    "            if u in {self.x_var, self.y_var}:\n",
    "                continue\n",
    "            \n",
    "            # Calculate edge indices\n",
    "            def get_edge_idx(src_idx, tgt_var):\n",
    "                nonlocal variables, p\n",
    "                tgt_idx = variables.index(tgt_var)\n",
    "                idx = edge_indices[(src_idx, tgt_idx)]\n",
    "                return idx\n",
    "\n",
    "            edges = [\n",
    "                get_edge_idx(u_idx, self.x_var),  # u->X\n",
    "                get_edge_idx(u_idx, self.y_var),  # u->Y\n",
    "                get_edge_idx(x_idx, u),           # X->u\n",
    "                get_edge_idx(y_idx, u)            # Y->u\n",
    "            ]\n",
    "\n",
    "            if None in edges:\n",
    "                raise ValueError(\"None edge indices\")\n",
    "                \n",
    "            # Merge embeddings\n",
    "            merged = self.node_merge([x[e] for e in edges])   # 4 * [1, 64] -> [1, 4 * 64] -> [1, 64]\n",
    "            node_embs.append(merged)\n",
    "        \n",
    "        node_logits = self.node_cls(torch.stack(node_embs))  # [p-2, 64] -> [p-2, 8]\n",
    "        # print('Node logits: ', node_logits)\n",
    "        return edge_logits, node_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, valid_dataset, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "class CausalLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, edge_weights, node_weights, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.edge_weights = edge_weights.to(self.device)\n",
    "        self.node_weights = node_weights.to(self.device)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def _compute_loss(self, batch):\n",
    "        edge_logits, node_logits = self.forward(batch)\n",
    "        \n",
    "        # Edge loss\n",
    "        edge_labels = batch['edge_labels'].squeeze(0).to(edge_logits.device)\n",
    "        edge_labels_idx = torch.argmax(edge_labels, dim=1)\n",
    "        edge_loss = F.cross_entropy(\n",
    "            edge_logits, edge_labels_idx,\n",
    "            weight=self.edge_weights.to(edge_logits.device)\n",
    "        )\n",
    "        \n",
    "        # Node loss\n",
    "        node_labels = batch['node_labels'].squeeze(0).to(node_logits.device)\n",
    "        node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "        node_loss = F.cross_entropy(\n",
    "            node_logits, node_labels_idx,\n",
    "            weight=self.node_weights.to(node_logits.device)\n",
    "        )\n",
    "\n",
    "        total_loss = edge_loss + node_loss\n",
    "        \n",
    "        return total_loss, edge_loss, node_loss\n",
    "    \n",
    "    def _compute_metrics(self, batch):\n",
    "        edge_logits, node_logits = self.forward(batch)\n",
    "\n",
    "        # Edge metrics: Balanced Accuracy\n",
    "        edge_labels = batch['edge_labels'].squeeze(0).to(edge_logits.device)\n",
    "        edge_labels_idx = torch.argmax(edge_labels, dim=1)\n",
    "        edge_preds = torch.argmax(edge_logits, dim=1)\n",
    "        edge_acc = (edge_preds == edge_labels_idx).float().mean()\n",
    "\n",
    "        # Node metrics: Balanced Accuracy\n",
    "        node_labels = batch['node_labels'].squeeze(0).to(node_logits.device)\n",
    "        node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "        node_preds = torch.argmax(node_logits, dim=1)\n",
    "        node_acc = (node_preds == node_labels_idx).float().mean()\n",
    "\n",
    "        return edge_acc, node_acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, loss_edge, loss_node = self._compute_loss(batch)\n",
    "        self.log_dict({\n",
    "            'train_loss': total_loss,\n",
    "            'train_edge_loss': loss_edge,\n",
    "            'train_node_loss': loss_node\n",
    "        }, prog_bar=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, loss_edge, loss_node = self._compute_loss(batch)\n",
    "        edge_acc, node_acc = self._compute_metrics(batch)\n",
    "        self.log_dict({\n",
    "            'val_loss': total_loss,\n",
    "            'val_edge_acc': edge_acc,\n",
    "            'val_node_acc': node_acc\n",
    "        }, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=10,  # Adjust based on total epochs\n",
    "            eta_min=1e-5\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def compute_class_weights(dataset):\n",
    "    edge_labels = []\n",
    "    node_labels = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        edge_labels.append(sample['edge_labels'])\n",
    "        node_labels.append(sample['node_labels'])\n",
    "    \n",
    "    # Process edge weights\n",
    "    edge_labels = torch.cat(edge_labels)\n",
    "    edge_counts = torch.sum(edge_labels, dim=0)\n",
    "    edge_weights = 1.0 / (edge_counts + 1e-5)  # Add epsilon to avoid division by zero\n",
    "    edge_weights = edge_weights / edge_weights.sum() * len(edge_counts)\n",
    "    \n",
    "    # Process node weights\n",
    "    node_labels = torch.cat(node_labels)\n",
    "    node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "    node_counts = torch.bincount(node_labels_idx)\n",
    "    node_weights = 1.0 / (node_counts + 1e-5)\n",
    "    node_weights = node_weights / node_weights.sum() * len(node_counts)\n",
    "    \n",
    "    return edge_weights, node_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets (top 5): ['09981', '08138', '30965', '01606', '00812']\n",
      "Test datasets (top 5): ['04552', '03154', '07222', '14344', '14242']\n"
     ]
    }
   ],
   "source": [
    "train_keys, test_keys = train_test_split(list(X_train_processed.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train datasets (top 5):\", train_keys[:5])\n",
    "print(\"Test datasets (top 5):\", test_keys[:5])\n",
    "\n",
    "X_train_split = {key: X_train[key] for key in train_keys}\n",
    "X_train_processed_split = {key: X_train_processed[key] for key in train_keys}\n",
    "y_train_split = {key: y_train[key] for key in train_keys}\n",
    "X_test_split = {key: X_train[key] for key in test_keys}\n",
    "X_test_processed_split = {key: X_train_processed[key] for key in test_keys}\n",
    "y_test_split = {key: y_train[key] for key in test_keys}\n",
    "\n",
    "train_dataset = CausalDataset(X_train_split, X_train_processed_split, y_train_split)\n",
    "test_dataset = CausalDataset(X_test_split, X_test_processed_split, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalModel(\n",
      "  (stem): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (edge_type_embed): Embedding(7, 64)\n",
      "  (self_attns): Sequential(\n",
      "    (0): SelfAttentionBlock(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): SelfAttentionBlock(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (edge_cls): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (node_merge): MergeBlock(\n",
      "    (linear): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation): GELU(approximate='none')\n",
      "  )\n",
      "  (node_cls): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CausalModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18800/18800 [01:53<00:00, 165.54it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_weights, node_weights = compute_class_weights(train_dataset)\n",
    "\n",
    "datamodule = CausalDataModule(train_dataset, test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(edge_weights.shape, node_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch keys: dict_keys(['edge_features', 'edge_types', 'node_labels', 'edge_labels', 'variables'])\n",
      "edge_features: shape=torch.Size([1, 90, 3, 1000]), dtype=torch.float32\n",
      "edge_types: shape=torch.Size([1, 90]), dtype=torch.int64\n",
      "node_labels: shape=torch.Size([1, 8, 8]), dtype=torch.int64\n",
      "edge_labels: shape=torch.Size([1, 90, 2]), dtype=torch.int64\n",
      "variables: [['Y'], ['1'], ['2'], ['X'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']]\n"
     ]
    }
   ],
   "source": [
    "# 获取 DataLoader\n",
    "train_loader = datamodule.train_dataloader()\n",
    "\n",
    "# 取一个 batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# 打印 batch 结构\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "\n",
    "# 详细打印数据\n",
    "for key, value in sample_batch.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', '1', '2', 'X', '4', '5', '6', '7', '8', '9']\n",
      "X Y\n",
      "3 0\n",
      "After stem: torch.Size([90, 64, 1000])\n",
      "After conv blocks:  torch.Size([90, 64, 1000])\n",
      "After pool:  torch.Size([90, 64])\n",
      "After edge type embed:  torch.Size([90, 64])\n",
      "After self attns:  torch.Size([90, 64])\n",
      "Edge logits:  tensor([[-0.3824,  0.0889],\n",
      "        [-0.7635,  0.1137],\n",
      "        [-0.2374,  0.5080],\n",
      "        [-0.8178,  0.0975],\n",
      "        [-0.8521,  0.0680],\n",
      "        [-0.4573,  0.0654],\n",
      "        [-0.8307,  0.0932],\n",
      "        [-0.7718,  0.0532],\n",
      "        [-0.7580,  0.1129],\n",
      "        [-0.8891,  0.1429],\n",
      "        [-0.5933,  0.1408],\n",
      "        [-0.4811, -0.0500],\n",
      "        [-0.6194,  0.1839],\n",
      "        [-0.6127,  0.1477],\n",
      "        [-0.5434,  0.2256],\n",
      "        [-0.6188,  0.1530],\n",
      "        [-0.5455,  0.2107],\n",
      "        [-0.5971,  0.1634],\n",
      "        [-0.9152,  0.2439],\n",
      "        [-0.3815,  0.1339],\n",
      "        [-0.4817, -0.0309],\n",
      "        [-0.5590,  0.1605],\n",
      "        [-0.5310,  0.2218],\n",
      "        [-0.4334,  0.1606],\n",
      "        [-0.4387, -0.0350],\n",
      "        [ 0.0316,  0.2297],\n",
      "        [-0.6188,  0.1996],\n",
      "        [-0.6944,  0.5378],\n",
      "        [-0.5147,  0.2344],\n",
      "        [-0.4866,  0.2783],\n",
      "        [-0.5335,  0.2783],\n",
      "        [-0.4894,  0.2766],\n",
      "        [-0.5210,  0.2304],\n",
      "        [-0.4929,  0.2676],\n",
      "        [-0.5073,  0.2365],\n",
      "        [-0.4541,  0.2650],\n",
      "        [-0.9469,  0.2206],\n",
      "        [-0.5989,  0.1511],\n",
      "        [-0.5833,  0.1879],\n",
      "        [-0.4825, -0.1392],\n",
      "        [-0.5211,  0.2071],\n",
      "        [-0.5968,  0.1452],\n",
      "        [-0.5714,  0.1765],\n",
      "        [-0.6100,  0.1622],\n",
      "        [-0.6652,  0.2198],\n",
      "        [-0.9600,  0.2540],\n",
      "        [-0.5726,  0.1287],\n",
      "        [-0.5591,  0.2184],\n",
      "        [-0.4848, -0.0276],\n",
      "        [-0.4775,  0.1989],\n",
      "        [-0.6210,  0.1330],\n",
      "        [-0.5767,  0.2085],\n",
      "        [-0.6127,  0.1501],\n",
      "        [-0.6347,  0.2266],\n",
      "        [-0.6994,  0.1918],\n",
      "        [-0.4175,  0.2095],\n",
      "        [-0.4920,  0.1329],\n",
      "        [-0.4792, -0.0820],\n",
      "        [-0.6174,  0.1886],\n",
      "        [-0.6438,  0.1543],\n",
      "        [-0.6762,  0.1355],\n",
      "        [-0.3865,  0.1955],\n",
      "        [-0.5843,  0.1615],\n",
      "        [-0.9529,  0.2419],\n",
      "        [-0.6460,  0.1918],\n",
      "        [-0.6459,  0.2177],\n",
      "        [-0.4973, -0.0261],\n",
      "        [-0.5310,  0.1532],\n",
      "        [-0.5687,  0.1960],\n",
      "        [-0.6373,  0.1863],\n",
      "        [-0.6600,  0.1493],\n",
      "        [-0.5160,  0.1534],\n",
      "        [-0.9312,  0.1986],\n",
      "        [-0.4290,  0.1711],\n",
      "        [-0.3662,  0.1408],\n",
      "        [-0.4105, -0.0589],\n",
      "        [-0.6517,  0.1863],\n",
      "        [-0.6138,  0.1676],\n",
      "        [-0.4914,  0.2008],\n",
      "        [-0.5978,  0.0869],\n",
      "        [-0.5898,  0.1648],\n",
      "        [-0.9318,  0.2334],\n",
      "        [-0.6212,  0.1586],\n",
      "        [-0.5931,  0.2157],\n",
      "        [-0.0815,  0.0389],\n",
      "        [-0.5850,  0.1024],\n",
      "        [-0.6699,  0.1784],\n",
      "        [-0.6023,  0.1328],\n",
      "        [-0.5027,  0.1804],\n",
      "        [-0.6054,  0.1484]], grad_fn=<AddmmBackward0>)\n",
      "Node logits:  tensor([[-0.3287, -0.1911,  0.2281,  0.2944, -0.1164,  0.2569,  0.4813,  0.4288],\n",
      "        [-0.3037, -0.0115,  0.3001,  0.4232, -0.0740,  0.4278,  0.3953,  0.2141],\n",
      "        [-0.2535,  0.0911,  0.3329,  0.4269, -0.1103,  0.4638,  0.2913,  0.1548],\n",
      "        [-0.3224,  0.0934,  0.3673,  0.4397,  0.0015,  0.5219,  0.4072,  0.0711],\n",
      "        [-0.3294, -0.1723,  0.2083,  0.3117, -0.1312,  0.2167,  0.4622,  0.4848],\n",
      "        [-0.3093,  0.0152,  0.3243,  0.4110, -0.0267,  0.4793,  0.4123,  0.1435],\n",
      "        [-0.3325, -0.0878,  0.2927,  0.3854, -0.0229,  0.3559,  0.4375,  0.2807],\n",
      "        [-0.2509, -0.0876,  0.3624,  0.3837,  0.0011,  0.3024,  0.5196,  0.2286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Edge logits shape: torch.Size([90, 2])\n",
      "Edge logits dtype: torch.float32\n",
      "Node logits shape: torch.Size([8, 8])\n",
      "Node logits dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = CausalModel()\n",
    "\n",
    "# 将 batch 输入模型\n",
    "edge_logits, node_logits = model(sample_batch)\n",
    "\n",
    "# 打印输出格式\n",
    "print(\"Edge logits shape:\", edge_logits.shape)\n",
    "print(\"Edge logits dtype:\", edge_logits.dtype)\n",
    "print(\"Node logits shape:\", node_logits.shape)\n",
    "print(\"Node logits dtype:\", node_logits.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode\n",
      "---------------------------------------------\n",
      "0 | model | CausalModel | 113 K  | eval\n",
      "---------------------------------------------\n",
      "113 K     Trainable params\n",
      "0         Non-trainable params\n",
      "113 K     Total params\n",
      "0.455     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "40        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 737/18800 [00:11<04:53, 61.46it/s, v_num=9, train_loss=nan.0, train_edge_loss=nan.0, train_node_loss=nan.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/amp.py:79\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py:241\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py:213\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py:73\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py:1097\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      3\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m pl_model \u001b[38;5;241m=\u001b[39m CausalLightningModule(\n\u001b[1;32m     11\u001b[0m     model,\n\u001b[1;32m     12\u001b[0m     edge_weights,\n\u001b[1;32m     13\u001b[0m     node_weights,\n\u001b[1;32m     14\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "pl_model = CausalLightningModule(\n",
    "    model,\n",
    "    edge_weights,\n",
    "    node_weights,\n",
    "    lr=1e-5\n",
    ")\n",
    "\n",
    "trainer.fit(pl_model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import typing\n",
    "import pickle\n",
    "import functools\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocess_kr import preprocess_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23500 23500 23500\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_pickle('data/X_train.pickle')\n",
    "y_train = pd.read_pickle('data/y_train.pickle')\n",
    "\n",
    "file_path = 'data/X_train_processed.npz'\n",
    "if os.path.exists(file_path):\n",
    "    X_train_processed = np.load(file_path, allow_pickle=True)\n",
    "else:\n",
    "    X_train_processed = preprocess_dict(X_train, n_workers=32)\n",
    "    np.savez_compressed(file_path, **X_train_processed)\n",
    "\n",
    "print(len(X_train), len(X_train_processed), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for DAG\n",
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        X_dict=typing.Dict[str, pd.DataFrame], \n",
    "        X_processed_dict=typing.Dict[str, pd.DataFrame], # Preprocessed data\n",
    "        y_dict=typing.Dict[str, pd.DataFrame], \n",
    "        x_var='X', \n",
    "        y_var='Y'\n",
    "    ):\n",
    "        self.X_dict = X_dict\n",
    "        self.X_processed_dict = X_processed_dict\n",
    "        self.y_dict = y_dict\n",
    "        self.ids = list(X_dict.keys())\n",
    "        self.x_var = x_var\n",
    "        self.y_var = y_var\n",
    "        self.adjacency_graph, self.adjacency_label = create_graph_label()\n",
    "        self.node_labels = [\n",
    "            'Confounder', 'Collider', 'Mediator', 'Independent',\n",
    "            'Cause of X', 'Consequence of X', 'Cause of Y', 'Consequence of Y',\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.ids[idx]\n",
    "        X_sample = self.X_dict[sample_id]                      # DataFrame (data: 1000 * num_nodes)\n",
    "        X_processed_sample = self.X_processed_dict[sample_id]  # numpy array (data: num_edges * 3 * 1000)\n",
    "        y_sample = self.y_dict[sample_id]                      # DataFrame (adjacency matrix: num_nodes * num_nodes)\n",
    "\n",
    "        variables = X_sample.columns.tolist()\n",
    "        edge_features, edge_types = self._process_edges(X_processed_sample, variables)\n",
    "        node_labels = self._process_node_labels(y_sample, variables)\n",
    "        edge_labels = self._process_edge_labels(y_sample, variables)\n",
    "\n",
    "        return {\n",
    "            'edge_features': edge_features,  # (num_edges, 3, 1000)\n",
    "            'edge_types': edge_types,        # (num_edges,)\n",
    "            'node_labels': node_labels,      # (num_nodes - 2, 8)    # For Node CLF\n",
    "            'edge_labels': edge_labels,      # (num_edges, 2)        # For Edge CLF\n",
    "            'variables': variables           # List[str]\n",
    "        }\n",
    "    \n",
    "    def _process_edges(self, X_processed_sample, variables):\n",
    "        edge_features = []\n",
    "        edge_types = []\n",
    "        for u in variables:\n",
    "            for v in variables:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                edge_types.append(self._get_edge_type(u, v))\n",
    "\n",
    "        edge_features = torch.tensor(X_processed_sample, dtype=torch.float32)\n",
    "        edge_types = torch.tensor(edge_types, dtype=torch.long)\n",
    "        return edge_features, edge_types\n",
    "\n",
    "    def _get_edge_type(self, u, v):\n",
    "        x, y = self.x_var, self.y_var\n",
    "        if u == x and v != y:      # u is X but v is not Y\n",
    "            return 0\n",
    "        elif u == y and v != x:    # u is Y but v is not X\n",
    "            return 1\n",
    "        elif u != y and v == x:    # u is not Y but v is X \n",
    "            return 2\n",
    "        elif u != x and v == y:    # u is not X but v is Y \n",
    "            return 3\n",
    "        elif u == x and v == y:    # u is X and v is Y\n",
    "            return 4\n",
    "        elif u == y and v == x:    # u is Y and v is X\n",
    "            return 5\n",
    "        else:                      # none of the above\n",
    "            return 6\n",
    "        \n",
    "    def _process_node_labels(self, y_sample, variables):\n",
    "        node_label_dict = get_labels(y_sample, self.adjacency_label)\n",
    "        \n",
    "        # 剔除 x_var 和 y_var，得到 nodes\n",
    "        nodes = [var for var in variables if var not in {self.x_var, self.y_var}]\n",
    "\n",
    "        # 初始化 0/1 矩阵，大小为 len(nodes) * len(self.node_labels)\n",
    "        node_label_matrix = np.zeros((len(nodes), len(self.node_labels)), dtype=int)\n",
    "\n",
    "        # 遍历 nodes，并根据 node_label_dict 填充矩阵\n",
    "        for i, node in enumerate(nodes):\n",
    "            if node in node_label_dict:\n",
    "                label = node_label_dict[node]\n",
    "                if label in self.node_labels:\n",
    "                    j = self.node_labels.index(label)  # 获取标签对应的索引\n",
    "                    node_label_matrix[i, j] = 1    # 设置为 1\n",
    "        return torch.tensor(node_label_matrix, dtype=torch.long)\n",
    "    \n",
    "    def _process_edge_labels(self, y_sample, variables):\n",
    "        edge_label_matrix = []\n",
    "        for u in variables:\n",
    "            for v in variables:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                label_vector = np.zeros(2, dtype=int)\n",
    "                if y_sample.loc[u, v] == 1:\n",
    "                    label_vector[1] = 1\n",
    "                else:\n",
    "                    label_vector[0] = 1\n",
    "                edge_label_matrix.append(label_vector)\n",
    "        edge_label_matrix = np.array(edge_label_matrix)\n",
    "        return torch.tensor(edge_label_matrix, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            channels, channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(8, channels)  # 8 groups for 64 channels\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + identity\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=64, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # add sequence dimension\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        return self.norm(x.squeeze(1) + attn_output)\n",
    "    \n",
    "class MergeBlock(nn.Module):\n",
    "    def __init__(self, input_dim=256, output_dim=64):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        concatenated = torch.cat(embeddings, dim=-1)\n",
    "        return self.activation(self.norm(self.linear(concatenated)))\n",
    "\n",
    "class CausalModel(nn.Module):\n",
    "    def __init__(self, x_var='X', y_var='Y', hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.x_var = x_var\n",
    "        self.y_var = y_var\n",
    "        \n",
    "        # Stem layer\n",
    "        self.stem = nn.Conv1d(3, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        self.conv_blocks = nn.Sequential(*[\n",
    "            ConvBlock(hidden_dim) for _ in range(5)\n",
    "        ])\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Edge type embedding\n",
    "        self.edge_type_embed = nn.Embedding(7, hidden_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attns = nn.Sequential(*[\n",
    "            SelfAttentionBlock(hidden_dim) for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Classification heads\n",
    "        self.edge_cls = nn.Linear(hidden_dim, 2)\n",
    "        self.node_merge = MergeBlock(4*hidden_dim)\n",
    "        self.node_cls = nn.Linear(hidden_dim, 8)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Unpack batch\n",
    "        edge_features = batch['edge_features'].squeeze(0)  # [E, 3, 1000]\n",
    "        edge_types = batch['edge_types'].squeeze(0)        # [E]\n",
    "        variables = batch['variables']                     # list of var names\n",
    "        variables = [item for sublist in variables for item in sublist]  # Flatten list\n",
    "        # print(variables)\n",
    "        # print(self.x_var, self.y_var)\n",
    "        # print(variables.index(self.x_var), variables.index(self.y_var))\n",
    "        \n",
    "        # Feature extraction\n",
    "        x = self.stem(edge_features)            # [E, 64, 1000]\n",
    "        # print('After stem:', x.shape)\n",
    "        x = self.conv_blocks(x)                 # [E, 64, 1000]\n",
    "        # print('After conv blocks: ', x.shape)\n",
    "        x = self.pool(x).squeeze(-1)            # [E, 64]\n",
    "        # print('After pool: ', x.shape)\n",
    "        \n",
    "        # Add edge type embeddings\n",
    "        x = x + self.edge_type_embed(edge_types)\n",
    "        # print('After edge type embed: ', x.shape)\n",
    "        \n",
    "        # Self-attention\n",
    "        x = self.self_attns(x)                  # [E, 64]\n",
    "        # print('After self attns: ', x.shape)\n",
    "        \n",
    "        # Edge classification\n",
    "        edge_logits = self.edge_cls(x)          # [E, 2]\n",
    "        # print('Edge logits: ', edge_logits)\n",
    "        \n",
    "        # Node classification\n",
    "        p = len(variables)\n",
    "        edges = [(u, v) for u in range(p) for v in range(p) if u != v]\n",
    "        edge_indices = {(u, v): idx for idx, (u, v) in enumerate(edges)}\n",
    "        node_embs = []\n",
    "        try:\n",
    "            x_idx = variables.index(self.x_var)\n",
    "            y_idx = variables.index(self.y_var)\n",
    "        except ValueError:\n",
    "            return edge_logits, torch.tensor([])\n",
    "        \n",
    "        for u_idx, u in enumerate(variables):\n",
    "            if u in {self.x_var, self.y_var}:\n",
    "                continue\n",
    "            \n",
    "            # Calculate edge indices\n",
    "            def get_edge_idx(src_idx, tgt_var):\n",
    "                nonlocal variables, p\n",
    "                tgt_idx = variables.index(tgt_var)\n",
    "                idx = edge_indices[(src_idx, tgt_idx)]\n",
    "                return idx\n",
    "\n",
    "            edges = [\n",
    "                get_edge_idx(u_idx, self.x_var),  # u->X\n",
    "                get_edge_idx(u_idx, self.y_var),  # u->Y\n",
    "                get_edge_idx(x_idx, u),           # X->u\n",
    "                get_edge_idx(y_idx, u)            # Y->u\n",
    "            ]\n",
    "\n",
    "            if None in edges:\n",
    "                raise ValueError(\"None edge indices\")\n",
    "                \n",
    "            # Merge embeddings\n",
    "            merged = self.node_merge([x[e] for e in edges])   # 4 * [1, 64] -> [1, 4 * 64] -> [1, 64]\n",
    "            node_embs.append(merged)\n",
    "        \n",
    "        node_logits = self.node_cls(torch.stack(node_embs))  # [p-2, 64] -> [p-2, 8]\n",
    "        # print('Node logits: ', node_logits)\n",
    "        return edge_logits, node_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, valid_dataset, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "class CausalLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, edge_weights, node_weights, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.edge_weights = edge_weights.to(self.device)\n",
    "        self.node_weights = node_weights.to(self.device)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def _compute_loss(self, batch):\n",
    "        edge_logits, node_logits = self.forward(batch)\n",
    "        \n",
    "        # Edge loss\n",
    "        edge_labels = batch['edge_labels'].squeeze(0).to(edge_logits.device)\n",
    "        edge_labels_idx = torch.argmax(edge_labels, dim=1)\n",
    "        edge_loss = F.cross_entropy(\n",
    "            edge_logits, edge_labels_idx,\n",
    "            weight=self.edge_weights.to(edge_logits.device)\n",
    "        )\n",
    "        \n",
    "        # Node loss\n",
    "        node_labels = batch['node_labels'].squeeze(0).to(node_logits.device)\n",
    "        node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "        node_loss = F.cross_entropy(\n",
    "            node_logits, node_labels_idx,\n",
    "            weight=self.node_weights.to(node_logits.device)\n",
    "        )\n",
    "\n",
    "        total_loss = edge_loss + node_loss\n",
    "        \n",
    "        return total_loss, edge_loss, node_loss\n",
    "    \n",
    "    def _compute_metrics(self, batch):\n",
    "        edge_logits, node_logits = self.forward(batch)\n",
    "\n",
    "        # Edge metrics: Accuracy\n",
    "        edge_labels = batch['edge_labels'].squeeze(0).to(edge_logits.device)\n",
    "        edge_labels_idx = torch.argmax(edge_labels, dim=1)\n",
    "        edge_preds = torch.argmax(edge_logits, dim=1)\n",
    "        edge_acc = (edge_preds == edge_labels_idx).float().mean()\n",
    "\n",
    "        # Node metrics: Accuracy\n",
    "        node_labels = batch['node_labels'].squeeze(0).to(node_logits.device)\n",
    "        node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "        node_preds = torch.argmax(node_logits, dim=1)\n",
    "        node_acc = (node_preds == node_labels_idx).float().mean()\n",
    "\n",
    "        return edge_acc, node_acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, loss_edge, loss_node = self._compute_loss(batch)\n",
    "        self.log_dict({\n",
    "            'train_loss': total_loss,\n",
    "            'train_edge_loss': loss_edge,\n",
    "            'train_node_loss': loss_node\n",
    "        }, prog_bar=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, loss_edge, loss_node = self._compute_loss(batch)\n",
    "        edge_acc, node_acc = self._compute_metrics(batch)\n",
    "        self.log_dict({\n",
    "            'val_loss': total_loss,\n",
    "            'val_edge_acc': edge_acc,\n",
    "            'val_node_acc': node_acc\n",
    "        }, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=10,  # Adjust based on total epochs\n",
    "            eta_min=1e-5\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def compute_class_weights(dataset):\n",
    "    edge_labels = []\n",
    "    node_labels = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        edge_labels.append(sample['edge_labels'])\n",
    "        node_labels.append(sample['node_labels'])\n",
    "    \n",
    "    # Process edge weights\n",
    "    edge_labels = torch.cat(edge_labels)\n",
    "    edge_counts = torch.sum(edge_labels, dim=0)\n",
    "    edge_weights = 1.0 / (edge_counts + 1e-5)  # Add epsilon to avoid division by zero\n",
    "    edge_weights = edge_weights / edge_weights.sum() * len(edge_counts)\n",
    "    \n",
    "    # Process node weights\n",
    "    node_labels = torch.cat(node_labels)\n",
    "    node_labels_idx = torch.argmax(node_labels, dim=1)\n",
    "    node_counts = torch.bincount(node_labels_idx)\n",
    "    node_weights = 1.0 / (node_counts + 1e-5)\n",
    "    node_weights = node_weights / node_weights.sum() * len(node_counts)\n",
    "    \n",
    "    return edge_weights, node_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets (top 5): ['09981', '08138', '30965', '01606', '00812']\n",
      "Test datasets (top 5): ['04552', '03154', '07222', '14344', '14242']\n"
     ]
    }
   ],
   "source": [
    "train_keys, test_keys = train_test_split(list(X_train_processed.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train datasets (top 5):\", train_keys[:5])\n",
    "print(\"Test datasets (top 5):\", test_keys[:5])\n",
    "\n",
    "X_train_split = {key: X_train[key] for key in train_keys}\n",
    "X_train_processed_split = {key: X_train_processed[key] for key in train_keys}\n",
    "y_train_split = {key: y_train[key] for key in train_keys}\n",
    "X_test_split = {key: X_train[key] for key in test_keys}\n",
    "X_test_processed_split = {key: X_train_processed[key] for key in test_keys}\n",
    "y_test_split = {key: y_train[key] for key in test_keys}\n",
    "\n",
    "train_dataset = CausalDataset(X_train_split, X_train_processed_split, y_train_split)\n",
    "test_dataset = CausalDataset(X_test_split, X_test_processed_split, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalModel(\n",
      "  (stem): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (activation): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (edge_type_embed): Embedding(7, 64)\n",
      "  (self_attns): Sequential(\n",
      "    (0): SelfAttentionBlock(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): SelfAttentionBlock(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (edge_cls): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (node_merge): MergeBlock(\n",
      "    (linear): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation): GELU(approximate='none')\n",
      "  )\n",
      "  (node_cls): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CausalModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18800/18800 [06:30<00:00, 48.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "edge_weights, node_weights = compute_class_weights(train_dataset)\n",
    "print(edge_weights.shape, node_weights.shape)\n",
    "datamodule = CausalDataModule(train_dataset, test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /hy-tmp/lightning_logs/version_0/checkpoints/epoch=18-step=357200.ckpt\n",
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/hy-tmp/lightning_logs/version_0/checkpoints' to '/hy-tmp/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | CausalModel | 113 K  | train\n",
      "----------------------------------------------\n",
      "113 K     Trainable params\n",
      "0         Non-trainable params\n",
      "113 K     Total params\n",
      "0.455     Total estimated model params size (MB)\n",
      "40        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /hy-tmp/lightning_logs/version_0/checkpoints/epoch=18-step=357200.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 18800/18800 [08:38<00:00, 36.23it/s, v_num=1, train_loss=0.498, train_edge_loss=0.326, train_node_loss=0.172, val_loss=1.270, val_edge_acc=0.818, val_node_acc=0.688]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 18800/18800 [08:38<00:00, 36.23it/s, v_num=1, train_loss=0.498, train_edge_loss=0.326, train_node_loss=0.172, val_loss=1.270, val_edge_acc=0.818, val_node_acc=0.688]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "pl_model = CausalLightningModule(\n",
    "    model,\n",
    "    edge_weights,\n",
    "    node_weights,\n",
    "    lr=1e-5\n",
    ")\n",
    "checkpoint_path = \"/hy-tmp/lightning_logs/version_0/checkpoints/epoch=18-step=357200.ckpt\"\n",
    "trainer.fit(pl_model, datamodule=datamodule, ckpt_path=checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
